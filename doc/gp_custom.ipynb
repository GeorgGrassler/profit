{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from profit.sur.backend.gp_functions import invert, nll\n",
    "from profit.sur.backend.kernels import kern_sqexp\n",
    "from profit.util.halton import halton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return x*np.cos(10*x)\n",
    "\n",
    "# Custom function to build GP matrix\n",
    "def build_K(xa, xb, hyp, K):\n",
    "    for i in np.arange(len(xa)):\n",
    "        for j in np.arange(len(xb)):\n",
    "            K[i, j] = kern_sqexp(xa[i], xb[j], hyp[0])\n",
    "\n",
    "noise_train = 0.01\n",
    "\n",
    "ntrain = 20\n",
    "xtrain = halton(1, ntrain)\n",
    "ftrain = f(xtrain)\n",
    "np.random.seed(0)\n",
    "ytrain = ftrain + noise_train*np.random.randn(ntrain, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP regression with fixed kernel hyperparameters\n",
    "hyp = [0.5, 1e-6]  # l and sig_noise**2\n",
    "\n",
    "K = np.empty((ntrain, ntrain))   # train-train\n",
    "build_K(xtrain, xtrain, hyp, K)  # writes inside K\n",
    "Ky = K + hyp[-1]*np.eye(ntrain)\n",
    "Kyinv = invert(Ky, 4, 1e-6)       # using gp_functions.invert\n",
    "\n",
    "ntest = 30\n",
    "xtest = np.linspace(0, 1, ntest)\n",
    "ftest = f(xtest)\n",
    "\n",
    "Ks = np.empty((ntrain, ntest))  # train-test\n",
    "Kss = np.empty((ntest, ntest))  # test-test\n",
    "build_K(xtrain, xtest, hyp, Ks)\n",
    "build_K(xtest, xtest, hyp, Kss)\n",
    "\n",
    "fmean = Ks.T.dot(Kyinv.dot(ytrain)) # predictive mean\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xtrain, ytrain, 'x')\n",
    "plt.plot(xtest, ftest, '-')\n",
    "plt.plot(xtest, fmean, '--')\n",
    "plt.legend(('training', 'reference', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative log likelihood over length scale\n",
    "ls = np.linspace(1e-3, 3, 50)\n",
    "nlls = np.array(\n",
    "    [nll([l, 0.00694534], xtrain, ytrain, 0) for l in ls]\n",
    "    ).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ls, nlls)\n",
    "plt.xlabel('l')\n",
    "plt.ylabel('- log p(y|l)')\n",
    "plt.title('Negative log-likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def nll_transform(log10hyp):\n",
    "    hyp = 10**log10hyp\n",
    "    return nll(hyp, xtrain, ytrain, 0)\n",
    "\n",
    "res = minimize(nll_transform, np.array([0, -6]), method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)\n",
    "print('[l,sig2] = ', 10**res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 50\n",
    "ns2 = 40\n",
    "\n",
    "log10l = np.linspace(res.x[0]-1, res.x[0]+1, nl)\n",
    "log10s2 = np.linspace(res.x[1]-1, res.x[1]+1, ns2)\n",
    "[Ll, Ls2] = np.meshgrid(log10l, log10s2)\n",
    "\n",
    "nlls = np.array(\n",
    "    [nll([10**ll, 10**ls2], xtrain, ytrain, 0) for ls2 in log10s2 for ll in log10l]\n",
    "    ).reshape([ns2, nl])\n",
    "\n",
    "# Do some cut for visualization\n",
    "maxval = 0.0\n",
    "nlls[nlls>maxval] = maxval\n",
    "\n",
    "plt.figure()\n",
    "plt.title('NLL')\n",
    "plt.contour(Ll, Ls2, nlls, levels=50)\n",
    "plt.plot(res.x[0], res.x[1], 'rx')\n",
    "plt.xlabel('log10 l^2')\n",
    "plt.ylabel('log10 sig_n^2')\n",
    "plt.colorbar()\n",
    "plt.legend(['optimum'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out priors to cut values\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def prior(hyp):\n",
    "    return sigmoid(hyp[0]-6)*sigmoid(hyp[-1]-6)\n",
    "\n",
    "x = np.logspace(-10, -5, 100)\n",
    "plt.semilogx(x, np.log(sigmoid(1e9*x - 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitee8223ec65594bc885f48f30722f6205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
