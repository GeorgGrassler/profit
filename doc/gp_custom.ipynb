{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from profit.sur.backend.gp_functions import invert, nll, predict_f, \\\n",
    "    get_marginal_variance, wld_get_marginal_variance\n",
    "from profit.sur.backend.kernels import kern_sqexp\n",
    "from profit.util.halton import halton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return x*np.cos(10*x)\n",
    "\n",
    "# Custom function to build GP matrix\n",
    "def build_K(xa, xb, hyp, K):\n",
    "    for i in np.arange(len(xa)):\n",
    "        for j in np.arange(len(xb)):\n",
    "            K[i, j] = kern_sqexp(xa[i], xb[j], hyp[0])\n",
    "\n",
    "noise_train = 0.01\n",
    "\n",
    "ntrain = 15\n",
    "xtrain = halton(1, ntrain)\n",
    "ftrain = f(xtrain)\n",
    "np.random.seed(0)\n",
    "ytrain = ftrain + noise_train*np.random.randn(ntrain, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP regression with fixed kernel hyperparameters\n",
    "hyp = [0.1, 1e-3]  # l and sig_noise**2\n",
    "\n",
    "K = np.empty((ntrain, ntrain))   # train-train\n",
    "build_K(xtrain, xtrain, hyp, K)  # writes inside K\n",
    "Ky = K + hyp[-1]*np.eye(ntrain)\n",
    "Kyinv = invert(Ky, 4, 1e-6)       # using gp_functions.invert\n",
    "\n",
    "ntest = 20\n",
    "xtest = np.linspace(0, 1, ntest)\n",
    "ftest = f(xtest)\n",
    "\n",
    "Ks = np.empty((ntrain, ntest))  # train-test\n",
    "Kss = np.empty((ntest, ntest))  # test-test\n",
    "build_K(xtrain, xtest, hyp, Ks)\n",
    "build_K(xtest, xtest, hyp, Kss)\n",
    "\n",
    "fmean = Ks.T.dot(Kyinv.dot(ytrain)) # predictive mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xtrain, ytrain, 'x')\n",
    "plt.plot(xtest, ftest, '-')\n",
    "plt.plot(xtest, fmean, '--')\n",
    "plt.legend(('training', 'reference', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ef, varf = predict_f(hyp, xtrain.reshape(-1, 1),\n",
    "                      ytrain.reshape(-1, 1), xtest.reshape(-1, 1), neig=8)# posterior \n",
    "# Estimation and variance\n",
    "varf = np.diag(varf)\n",
    "\n",
    "# we keep only the diag because the variance is on it, the other terms are covariance\n",
    "\n",
    "\n",
    "plt.plot(xtrain, ytrain, 'kx')\n",
    "plt.plot(xtest, ftest, 'm-')\n",
    "plt.plot(xtest, fmean, 'r--')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-1.5, 1])\n",
    "plt.title('Gaussian Process with '+ str(ntrain) + ' observation(s)')\n",
    "plt.legend(('training', 'reference', 'prediction'))\n",
    "\n",
    "\n",
    "\n",
    "plt.fill_between(xtest, # x\n",
    "                 (fmean.flatten() + 2 * np.sqrt(varf)), # y1\n",
    "                 (fmean.flatten() - 2 * np.sqrt(varf))) # y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative log likelihood over length scale\n",
    "ls = np.linspace(1e-3, 3, 50)\n",
    "nlls = np.array(\n",
    "    [nll([l, 0.00694534], xtrain, ytrain, 0) for l in ls]\n",
    "    ).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ls, nlls)\n",
    "plt.xlabel('l')\n",
    "plt.ylabel('- log p(y|l)')\n",
    "plt.title('Negative log-likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def nll_transform(log10hyp):\n",
    "    hyp = 10**log10hyp\n",
    "    return nll(hyp, xtrain, ytrain, 0)\n",
    "\n",
    "res = minimize(nll_transform, np.array([0, -6]), method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)\n",
    "#hyp = 10**res.x\n",
    "print('[l,sig2] = ', 10**res.x)\n",
    "print('[log_l, log_s2] = ', res.x)\n",
    "log_l = res.x[0]\n",
    "log_s2= res.x[1]\n",
    "log_hyp = [log_l, log_s2]\n",
    "\n",
    "new_hyp = [10**res.x[0], 10**res.x[1]]\n",
    "hess_inv = res.hess_inv\n",
    "print(\"\\nhess_inv = \", hess_inv)\n",
    "print(\"\\nhess = \", invert(hess_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 50\n",
    "ns2 = 40\n",
    "\n",
    "log10l = np.linspace(res.x[0]-1, res.x[0]+1, nl)\n",
    "log10s2 = np.linspace(res.x[1]-1, res.x[1]+1, ns2)\n",
    "[Ll, Ls2] = np.meshgrid(log10l, log10s2)\n",
    "\n",
    "nlls = np.array(\n",
    "    [nll([10**ll, 10**ls2], xtrain, ytrain, 0) for ls2 in log10s2 for ll in log10l]\n",
    "    ).reshape([ns2, nl])\n",
    "    \n",
    "# Do some cut for visualization\n",
    "maxval = 0.0\n",
    "nlls[nlls>maxval] = maxval\n",
    "\n",
    "plt.figure()\n",
    "plt.title('NLL')\n",
    "plt.contour(Ll, Ls2, nlls, levels=50)\n",
    "plt.plot(res.x[0], res.x[1], 'rx')\n",
    "plt.xlabel('log10 l^2')\n",
    "plt.ylabel('log10 sig_n^2')\n",
    "plt.colorbar()\n",
    "plt.legend(['optimum'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out priors to cut values\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def prior(hyp):\n",
    "    return sigmoid(hyp[0]-6)*sigmoid(hyp[-1]-6)\n",
    "\n",
    "x = np.logspace(-10, -5, 100)\n",
    "plt.semilogx(x, np.log(sigmoid(1e9*x - 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profit.sur.backend.gp_functions import k\n",
    "\n",
    "def dk_logdl(xa, xb, l): # derivative of the kernel w.r.t log lengthscale\n",
    "    dk_dl = ((xa - xb)**2.0 * np.exp(-(xa-xb)**2.0/(2 * l**2))) / l**3 \n",
    "    dk_logdl = dk_dl * np.log(10) * 10**log_l # from log lengthscale to lengthscale\n",
    "    return dk_logdl\n",
    "    \n",
    "\n",
    "def dkdl(xa, xb, l): # derivative of the kernel w.r.t lengthscale\n",
    "    dk_dl = ((xa - xb)**2.0 * np.exp(-(xa-xb)**2.0/(2 * l**2))) / l**3 \n",
    "    return dk_dl"
   ]
  },
  {
   "cell_type": "heading",
   "level": 1,
   "metadata": {},
   "source": [
    "Method 1 : log Kernel derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_K = np.empty((ntrain, ntrain))\n",
    "for i in np.arange(len(xtrain)):\n",
    "    \n",
    "    for j in np.arange(len(xtrain)):\n",
    "        log_K[i, j] = k(xtrain[i], xtrain[j], log_hyp[0])\n",
    "\n",
    "\n",
    "log_K_star = np.empty((ntest, ntrain))\n",
    "for i in np.arange(len(xtest)):\n",
    "    for j in np.arange(len(xtrain)):\n",
    "        log_K_star[i, j] = k(xtest[i], xtrain[j], log_hyp[0])\n",
    "\n",
    "\n",
    "log_K_prime = np.empty((ntrain, ntrain))\n",
    "for i in np.arange(len(xtrain)):\n",
    "    for j in np.arange(len(xtrain)):\n",
    "        log_K_prime[i, j] = dk_logdl(xtrain[i], xtrain[j], log_hyp[0])\n",
    "\n",
    "\n",
    "log_K_star_prime = np.empty((ntest, ntrain))\n",
    "for i in np.arange(len(xtest)):\n",
    "    for j in np.arange(len(xtrain)):\n",
    "        log_K_star_prime[i, j] = dk_logdl(xtest[i], xtrain[j], log_hyp[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# K.shape = (20, 20)\n",
    "# K_prime.shape = (20, 20)\n",
    "\n",
    "# K_star.shape = (10, 20)\n",
    "# K_star_prime.shape = (10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "log_Ky = log_K + log_hyp[-1]*np.eye(ntrain)\n",
    "log_Kyinv = invert(log_Ky, 4, 1e-6)\n",
    "\n",
    "alpha = np.dot(Kyinv, ytrain) # RW p17 paragraph 4\n",
    "\n",
    "log_dalpha_dl = -Kyinv.dot(log_K_prime)\\\n",
    "    .dot(Kyinv)\\\n",
    "    .dot(ytrain)\n",
    "\n",
    "log_dalpha_ds = -Kyinv.dot(np.log(10) * 10**log_s2 * np.eye(ntrain)).dot(Kyinv).dot(ytrain) # - Kyinv x ln(10) x 10^log_sigma x I x Kyinv x ytrain \n",
    "\n",
    "log_dm = np.empty((ntest,len(hyp), 1))\n",
    "\n",
    "for nb_hyp in range(len(hyp)):\n",
    "    if nb_hyp == 0 :\n",
    "        log_dm[:,nb_hyp,:] = np.dot(log_K_star_prime, alpha) -\\\n",
    "                         np.dot(log_K_star, log_dalpha_dl) \n",
    "    else : \n",
    "        log_dm[:,nb_hyp,:] = np.dot(log_K_star, log_dalpha_ds)\n",
    "\n",
    "print(\"\\n\\n\\ndm :\",log_dm.shape, \"\\n\\n\")\n",
    "#print(dm)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sigma = invert(hess_inv) # define the sigma matrix as the inverse of hess_inv\n",
    "V = varf # set V as the result of the predict_f diagonal  \n",
    "\n",
    "#print(\"\\nsigma shape : \", log_sigma.shape)\n",
    "#print(log_sigma)\n",
    "#print(\"dm.shape : \", log_dm.shape)\n",
    "\n",
    "\n",
    "log_dm_transpose = np.empty((ntest, 1, len(log_hyp)))\n",
    "log_dmT_dot_sigma = np.empty((ntest, 1, len(log_hyp)))\n",
    "log_dmT_dot_sigma_dot_dm = np.empty((ntest, 1))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(ntest):\n",
    "    log_dm_transpose[i] = log_dm[i].T\n",
    "    #print(\"\\n\\ndm.t\",i,' : ', log_dm_transpose[i])\n",
    "    log_dmT_dot_sigma[i] = log_dm_transpose[i].dot(log_sigma)\n",
    "    #print(\"sigma = \", log_sigma)\n",
    "    #print(\"dmT_dot_sigma\",i,\" : \", log_dmT_dot_sigma[i])\n",
    "    log_dmT_dot_sigma_dot_dm[i] = log_dmT_dot_sigma[i].dot(log_dm[i])\n",
    "    #print(\"dmT_dot_sigma_dot_dm\",i,\" : \", dmT_dot_sigma_dot_dm[i])\n",
    "    \n",
    "# print(\"dm_transpose :\", dm_transpose.shape)\n",
    "# print(\"\\ndmT_dot_sigma \", dmT_dot_sigma.shape)\n",
    "# print(\"dmT_dot_sigma_dot_dm \", dmT_dot_sigma_dot_dm.shape)\n",
    "# print(\"V \", V.shape)\n",
    "\n",
    "\n",
    " \n",
    "log_V_tild = V.reshape((ntest,1)) + log_dmT_dot_sigma_dot_dm # Osborne et al. (2012) Active learning eq.19 \n",
    "\n",
    "\n",
    "print(\"V_tild.shape \", log_V_tild.shape)\n",
    "print(\"\\n\\n\\tMarginal variance\\n\\n\", log_V_tild )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "heading",
   "level": 1,
   "metadata": {},
   "source": [
    "Method 2 : transfom the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tac = time.time()\n",
    "marginal_variance = get_marginal_variance(hess_inv, new_hyp, ntrain, ntest, xtrain, xtest, \n",
    "                                          Kyinv, ytrain, varf,True)\n",
    "tuc = time.time()\n",
    "\n",
    "log_time = tuc - tac \n",
    "print(log_time, \" second\\n-> \", log_time * 1000, \" ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(xtrain, ytrain, 'kx')\n",
    "plt.plot(xtest, ftest, 'm-')\n",
    "plt.plot(xtest, fmean, 'r--')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-2, 2])\n",
    "plt.title('Gaussian Process with '+ str(ntrain) + ' observation(s)')\n",
    "plt.legend(('training', 'reference', 'prediction'))\n",
    "\n",
    "\n",
    "\n",
    "plt.fill_between(xtest, # x\n",
    "                 (fmean.flatten() + 2 * np.sqrt(marginal_variance.flatten())), # y1\n",
    "                 (fmean.flatten() - 2 * np.sqrt(marginal_variance.flatten()))) # y2\n"
   ]
  },
  {
   "cell_type": "heading",
   "level": 1,
   "metadata": {},
   "source": [
    "Trying without log distribution (WLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wld_get_marginal_variance(wld_hess_inv, wld_hyp, ntrain, ntest, xtrain, xtest,\n",
    "                              Kyinv, ytrain, varf, plot_result = False):\n",
    "\n",
    "\n",
    "\n",
    "    ######################### Build needed Kernel Matrix #########################\n",
    "    wld_K = np.empty((ntrain, ntrain))\n",
    "    for i in np.arange(len(xtrain)):\n",
    "        for j in np.arange(len(xtrain)):\n",
    "            wld_K[i, j] = k(xtrain[i], xtrain[j], wld_hyp[0])\n",
    "\n",
    "\n",
    "    wld_K_star = np.empty((ntest, ntrain))\n",
    "    for i in np.arange(len(xtest)):\n",
    "        for j in np.arange(len(xtrain)):\n",
    "            wld_K_star[i, j] = k(xtest[i], xtrain[j], wld_hyp[0])\n",
    "\n",
    "\n",
    "    wld_K_prime = np.empty((ntrain, ntrain))\n",
    "    for i in np.arange(len(xtrain)):\n",
    "        for j in np.arange(len(xtrain)):\n",
    "            wld_K_prime[i, j] = dkdl(xtrain[i], xtrain[j], wld_hyp[0])\n",
    "\n",
    "\n",
    "    wld_K_star_prime = np.empty((ntest, ntrain))\n",
    "    for i in np.arange(len(xtest)):\n",
    "        for j in np.arange(len(xtrain)):\n",
    "            wld_K_star_prime[i, j] = dkdl(xtest[i], xtrain[j], wld_hyp[0])\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    wld_alpha = np.dot(Kyinv, ytrain) # RW p17 paragraph 4\n",
    "\n",
    "    wld_dalpha_dl = -Kyinv.dot(wld_K_prime)\\\n",
    "        .dot(Kyinv)\\\n",
    "        .dot(ytrain)\n",
    "\n",
    "    wld_dalpha_ds = -Kyinv.dot(np.eye(ntrain)).dot(Kyinv).dot(ytrain) # - Kyinv x I x Kyinv x ytrain\n",
    "\n",
    "    wld_dm = np.empty((ntest,len(wld_hyp), 1))\n",
    "\n",
    "\n",
    "    for nb_hyp in range(len(wld_hyp)):\n",
    "        if nb_hyp == 0 :\n",
    "            wld_dm[:,nb_hyp,:] = np.dot(wld_K_star_prime, wld_alpha) -\\\n",
    "                             np.dot(wld_K_star, wld_dalpha_dl)\n",
    "        else :\n",
    "            wld_dm[:,nb_hyp,:] = np.dot(wld_K_star, wld_dalpha_ds)\n",
    "\n",
    "    V = varf # set V as the result of the predict_f diagonal\n",
    "    wld_sigma = invert(wld_hess_inv)\n",
    "    print(\"sigma \", wld_sigma)\n",
    "\n",
    "    wld_dm_transpose = np.empty((ntest, 1, len(wld_hyp)))\n",
    "    wld_dmT_dot_sigma = np.empty((ntest, 1, len(wld_hyp)))\n",
    "    wld_dmT_dot_sigma_dot_dm = np.empty((ntest, 1))\n",
    "\n",
    "    for i in range(ntest):\n",
    "        wld_dm_transpose[i] = wld_dm[i].T\n",
    "        wld_dmT_dot_sigma[i] = wld_dm_transpose[i].dot(wld_sigma)\n",
    "        wld_dmT_dot_sigma_dot_dm[i] = wld_dmT_dot_sigma[i].dot(wld_dm[i])\n",
    "\n",
    "    wld_V_tild = V.reshape((ntest,1)) + wld_dmT_dot_sigma_dot_dm # Osborne et al. (2012) Active learning eq.19\n",
    "\n",
    "    if plot_result == True :\n",
    "        print(\"The marginal Variance has a shape of \", wld_V_tild.shape)\n",
    "        print(\"\\n\\n\\tMarginal variance\\n\\n\", wld_V_tild )\n",
    "\n",
    "    return wld_V_tild\n",
    "\n",
    "\n",
    "\n",
    "result = minimize(nll, hyp, args=(xtrain, ytrain), method='L-BFGS-B') \n",
    "# Got Identity matrix as hessian with L-BFGS-B\n",
    "print(\"\\n\\n\", result)\n",
    "wld_hyp = result.x\n",
    "wld_hess_inv = result.hess_inv.todense()\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "wld_marginal_variance = wld_get_marginal_variance(wld_hess_inv, wld_hyp, ntrain, \n",
    "                                 ntest, xtrain, xtest, Kyinv, ytrain, varf, True)\n",
    "\n",
    "tac = time.time()\n",
    "\n",
    "wld_time = tac - tic \n",
    "print(wld_time, \" second\\n-> \", wld_time * 1000, \" ms\")"
   ]
  },
  {
   "cell_type": "heading",
   "level": 1,
   "metadata": {},
   "source": [
    "Comparison between both method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time = np.abs(wld_time - log_time)\n",
    "diff = []\n",
    "res = 0\n",
    "for i in range(len(marginal_variance)):\n",
    "    res += np.abs(marginal_variance[i] - wld_marginal_variance[i])/wld_marginal_variance[i]\n",
    "\n",
    "res *= 100\n",
    "res /= len(marginal_variance)\n",
    "\n",
    "print(\"\\tThere is a value difference of \",round(res.item(), 1), \" % \")\n",
    "print(\"\\tWld method time : \", round(wld_time*1000, 2), \" ms\")\n",
    "print(\"\\tLog method time : \", round(log_time*1000, 2), \" ms\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xtrain, ytrain, 'kx')\n",
    "plt.plot(xtest, ftest, 'm-')\n",
    "plt.plot(xtest, fmean, 'r--')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-2, 2])\n",
    "plt.title('Gaussian Process with '+ str(ntrain) + ' observation(s)')\n",
    "plt.legend(('training', 'reference', 'prediction'))\n",
    "\n",
    "\n",
    "\n",
    "plt.fill_between(xtest, # x\n",
    "                 (fmean.flatten() + 2 * np.sqrt(wld_marginal_variance.flatten())), # y1\n",
    "                 (fmean.flatten() - 2 * np.sqrt(wld_marginal_variance.flatten()))) # y2\n"
   ]
  },
  {
   "cell_type": "heading",
   "level": 1,
   "metadata": {},
   "source": [
    "Playing with l and s2"
   ]
  },
  {
   "cell_type": "heading",
   "level": 4,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 50\n",
    "ns2 = 40\n",
    "\n",
    "\n",
    "log10s2 = np.linspace(res.x[1]-1, res.x[1]+4, ns2)\n",
    "[Ll, Ls2] = np.meshgrid(log10l, log10s2)\n",
    "\n",
    "nlls = np.array(\n",
    "    [nll([10**log_l, 10**ls2], xtrain, ytrain, 0) for ls2 in log10s2]\n",
    "    ).reshape([ns2, 1])\n",
    "    \n",
    "# Do some cut for visualization\n",
    "maxval = 0.0\n",
    "nlls[nlls>maxval] = maxval\n",
    "\n",
    "plt.figure()\n",
    "plt.title('NLL with fixed lengthscale')\n",
    "plt.plot(res.x[0], res.x[1], 'rx')\n",
    "plt.plot(log10s2, nlls)\n",
    "plt.xlabel('log10 s^^')\n",
    "plt.ylabel('nll')\n",
    "plt.legend(['optimum'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 50\n",
    "ns2 = 40\n",
    "\n",
    "log10l = np.linspace(res.x[0]-1, res.x[0]+1, nl)\n",
    "\n",
    "[Ll, Ls2] = np.meshgrid(log10l, log10s2)\n",
    "\n",
    "nlls = np.array(\n",
    "    [nll([10**ll, log_s2], xtrain, ytrain, 0) for ll in log10l]\n",
    "    ).reshape([nl, 1])\n",
    "    \n",
    "# Do some cut for visualization\n",
    "maxval = 0.0\n",
    "nlls[nlls>maxval] = maxval\n",
    "\n",
    "plt.figure()\n",
    "plt.title('NLL')\n",
    "\n",
    "plt.plot(res.x[0], res.x[1], 'rx')\n",
    "plt.plot(log10l, nlls)\n",
    "plt.xlabel('log10 l^2')\n",
    "plt.ylabel('nll')\n",
    "plt.legend(['optimum'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nll_fixed_l(new_s2, new_l): \n",
    "    return nll([new_l, new_s2], xtrain, ytrain, 0)\n",
    "\n",
    "lengthscale = np.linspace(0.1,10,100)\n",
    "sigma_noise = []\n",
    "\n",
    "for i in lengthscale:\n",
    "    opti = minimize(nll_fixed_l,1e-8, args=(i), method='BFGS')\n",
    "    sigma_noise.append(opti.x.item())\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Optimized s2 with l fixed')\n",
    "plt.semilogx(lengthscale, sigma_noise)\n",
    "plt.xlabel('lengthscale')\n",
    "plt.ylabel('optimal sigmanoise')\n",
    "plt.savefig('fixed_l')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_fixed_s2(new_l, new_s2): \n",
    "    return nll([new_l, new_s2], xtrain, ytrain, 0)\n",
    "\n",
    "lengthscale = []\n",
    "sigma_noise = np.linspace(1e-10, 1, 100)\n",
    "\n",
    "for i in sigma_noise:\n",
    "    opti = minimize(nll_fixed_s2,0.1, args=(i), method='BFGS')\n",
    "    lengthscale.append(opti.x.item())\n",
    "    #print(\"\\n\\n\\n\",opti)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Optimized l with s2 fixed')\n",
    "plt.plot(sigma_noise, lengthscale)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('sigma noise')\n",
    "plt.ylabel('optimal lengthscale')\n",
    "plt.savefig('fixed_s2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "heading",
   "level": 1,
   "metadata": {},
   "source": [
    "Active Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitee8223ec65594bc885f48f30722f6205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
