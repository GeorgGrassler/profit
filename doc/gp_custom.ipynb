{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from profit.sur.backend.gp_functions import invert, nll, predict_f\n",
    "from profit.sur.backend.kernels import kern_sqexp\n",
    "from profit.util.halton import halton\n",
    "from numpy.linalg import multi_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return x*np.cos(10*x)\n",
    "\n",
    "# Custom function to build GP matrix\n",
    "def build_K(xa, xb, hyp, K):\n",
    "    for i in np.arange(len(xa)):\n",
    "        for j in np.arange(len(xb)):\n",
    "            K[i, j] = kern_sqexp(xa[i], xb[j], hyp[0])\n",
    "\n",
    "noise_train = 0.01\n",
    "\n",
    "ntrain = 20\n",
    "xtrain = halton(1, ntrain)\n",
    "ftrain = f(xtrain)\n",
    "np.random.seed(0)\n",
    "ytrain = ftrain + noise_train*np.random.randn(ntrain, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP regression with fixed kernel hyperparameters\n",
    "hyp = [0.5, 1e-6]  # l and sig_noise**2\n",
    "\n",
    "K = np.empty((ntrain, ntrain))   # train-train\n",
    "build_K(xtrain, xtrain, hyp, K)  # writes inside K\n",
    "Ky = K + hyp[-1]*np.eye(ntrain)\n",
    "Kyinv = invert(Ky, 4, 1e-6)       # using gp_functions.invert\n",
    "\n",
    "ntest = 10\n",
    "xtest = np.linspace(0, 1, ntest)\n",
    "ftest = f(xtest)\n",
    "\n",
    "Ks = np.empty((ntrain, ntest))  # train-test\n",
    "Kss = np.empty((ntest, ntest))  # test-test\n",
    "build_K(xtrain, xtest, hyp, Ks)\n",
    "build_K(xtest, xtest, hyp, Kss)\n",
    "\n",
    "fmean = Ks.T.dot(Kyinv.dot(ytrain)) # predictive mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xtrain, ytrain, 'x')\n",
    "plt.plot(xtest, ftest, '-')\n",
    "plt.plot(xtest, fmean, '--')\n",
    "plt.legend(('training', 'reference', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ef, varf = predict_f(hyp, xtrain.reshape(-1, 1),\n",
    "                      ytrain.reshape(-1, 1), xtest.reshape(-1, 1), neig=8)# posterior Estimation and variance\n",
    "varf = np.diag(varf)\n",
    "\n",
    "# we keep only the diag because the variance is on it, the other terms are covariance\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(xtrain, ytrain, 'kx')\n",
    "plt.plot(xtest, ftest, 'm-')\n",
    "plt.plot(xtest, fmean, 'r--')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-1.5, 1])\n",
    "plt.title('Gaussian Process with '+ str(ntrain) + ' observation(s)')\n",
    "plt.legend(('training', 'reference', 'prediction'))\n",
    "\n",
    "\n",
    "\n",
    "plt.fill_between(xtest, # x\n",
    "                 (fmean.flatten() + 2 * np.sqrt(varf)), # y1\n",
    "                 (fmean.flatten() - 2 * np.sqrt(varf))) # y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative log likelihood over length scale\n",
    "ls = np.linspace(1e-3, 3, 50)\n",
    "nlls = np.array(\n",
    "    [nll([l, 0.00694534], xtrain, ytrain, 0) for l in ls]\n",
    "    ).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ls, nlls)\n",
    "plt.xlabel('l')\n",
    "plt.ylabel('- log p(y|l)')\n",
    "plt.title('Negative log-likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def nll_transform(log10hyp):\n",
    "    hyp = 10**log10hyp\n",
    "    return nll(hyp, xtrain, ytrain, 0)\n",
    "\n",
    "res = minimize(nll_transform, np.array([0, -6]), method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)\n",
    "print('[l,sig2] = ', 10**res.x)\n",
    "hess_inv = res.hess_inv\n",
    "print(\"\\nhess_inv = \", hess_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 50\n",
    "ns2 = 40\n",
    "\n",
    "log10l = np.linspace(res.x[0]-1, res.x[0]+1, nl)\n",
    "log10s2 = np.linspace(res.x[1]-1, res.x[1]+1, ns2)\n",
    "[Ll, Ls2] = np.meshgrid(log10l, log10s2)\n",
    "\n",
    "nlls = np.array(\n",
    "    [nll([10**ll, 10**ls2], xtrain, ytrain, 0) for ls2 in log10s2 for ll in log10l]\n",
    "    ).reshape([ns2, nl])\n",
    "\n",
    "# Do some cut for visualization\n",
    "maxval = 0.0\n",
    "nlls[nlls>maxval] = maxval\n",
    "\n",
    "plt.figure()\n",
    "plt.title('NLL')\n",
    "plt.contour(Ll, Ls2, nlls, levels=50)\n",
    "plt.plot(res.x[0], res.x[1], 'rx')\n",
    "plt.xlabel('log10 l^2')\n",
    "plt.ylabel('log10 sig_n^2')\n",
    "plt.colorbar()\n",
    "plt.legend(['optimum'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out priors to cut values\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def prior(hyp):\n",
    "    return sigmoid(hyp[0]-6)*sigmoid(hyp[-1]-6)\n",
    "\n",
    "x = np.logspace(-10, -5, 100)\n",
    "plt.semilogx(x, np.log(sigmoid(1e9*x - 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from profit.sur.backend.gp_functions import k\n",
    "\n",
    "def dkdl(xa, xb, l): # derivative of the kernel w.r.t lengthscale\n",
    "    return ((xa - xb)**2.0 * np.exp(-(xa-xb)**2.0/(2 * l**2))) / l**3\n",
    "\n",
    "\n",
    "K = np.empty((ntrain, ntrain))\n",
    "for i in np.arange(len(xtrain)):\n",
    "    for j in np.arange(len(xtrain)):\n",
    "        K[i, j] = k(xtrain[i], xtrain[j], hyp[0])\n",
    "\n",
    "\n",
    "K_star = np.empty((ntest, ntrain))\n",
    "for i in np.arange(len(xtest)):\n",
    "    for j in np.arange(len(xtrain)):\n",
    "        K_star[i, j] = k(xtest[i], xtrain[j], hyp[0])\n",
    "\n",
    "\n",
    "K_prime = np.empty((ntrain, ntrain))\n",
    "for i in np.arange(len(xtrain)):\n",
    "    for j in np.arange(len(xtrain)):\n",
    "        K_prime[i, j] = dkdl(xtrain[i], xtrain[j], hyp[0])\n",
    "\n",
    "\n",
    "K_star_prime = np.empty((ntest, ntrain))\n",
    "for i in np.arange(len(xtest)):\n",
    "    for j in np.arange(len(xtrain)):\n",
    "        K_star_prime[i, j] = dkdl(xtest[i], xtrain[j], hyp[0])\n",
    "\n",
    "# K.shape = (20, 20)\n",
    "# K_prime.shape = (20, 20)\n",
    "\n",
    "# K_star.shape = (10, 20)\n",
    "# K_star_prime.shape = (10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alpha = np.dot(Kyinv, ytrain) # RW p17 paragraph 4\n",
    "\n",
    "dalpha_dl = -Kyinv.dot(K_prime)\\\n",
    "    .dot(Kyinv)\\\n",
    "    .dot(ytrain)\n",
    "\n",
    "dalpha_ds = -Kyinv.dot(np.eye(ntrain)).dot(Kyinv).dot(ytrain) # - Kyinv x I x Kyinv x ytrain \n",
    "\n",
    "dm = np.empty((ntest,len(hyp), 1))\n",
    "\n",
    "for nb_hyp in range(len(hyp)):\n",
    "    if nb_hyp == 0 :\n",
    "        dm[:,nb_hyp,:] = np.dot(K_star_prime, alpha) -\\\n",
    "                         np.dot(K_star, dalpha_dl) \n",
    "    else : \n",
    "        dm[:,nb_hyp,:] = np.dot(K_star, dalpha_ds)\n",
    "\n",
    "print(\"\\n\\n\\ndm :\",dm.shape, \"\\n\\n\")\n",
    "print(dm)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = invert(hess_inv) # define the sigma matrix as the inverse of hess_inv\n",
    "V = varf # set V as the result of the predict_f diagonal  \n",
    "\n",
    "print(\"\\nsigma shape : \", sigma.shape)\n",
    "print(sigma)\n",
    "print(\"dm.shape : \", dm.shape)\n",
    "\n",
    "\n",
    "\n",
    "dm_transpose = np.empty((ntest, 1, len(hyp)))\n",
    "dmT_dot_sigma = np.empty((ntest, 1, len(hyp)))\n",
    "dmT_dot_sigma_dot_dm = np.empty((ntest, 1))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(ntest):\n",
    "    dm_transpose[i] = dm[i].T\n",
    "    print(\"\\n\\ndm.t\",i,' : ', dm_transpose[i])\n",
    "    dmT_dot_sigma[i] = dm_transpose[i].dot(sigma)\n",
    "    print(\"dmT_dot_sigma\",i,\" : \",dmT_dot_sigma[i])\n",
    "    dmT_dot_sigma_dot_dm[i] = dmT_dot_sigma[i].dot(dm[i])\n",
    "    print(\"dmT_dot_sigma_dot_dm\",i,\" : \", dmT_dot_sigma_dot_dm[i])\n",
    "    \n",
    "print(\"dm_transpose :\", dm_transpose.shape)\n",
    "print(\"\\ndmT_dot_sigma \", test.shape)\n",
    "print(\"dmT_dot_sigma_dot_dm \", dmT_dot_sigma_dot_dm.shape)\n",
    "print(\"V \", V.shape)\n",
    "\n",
    "\n",
    " \n",
    "V_tild = V.reshape((ntest,1)) + dmT_dot_sigma_dot_dm # Osborne et al. (2012) Active learning eq.19 \n",
    "\n",
    "\n",
    "print(\"V_tild.shape \", V_tild.shape)\n",
    "print(\"\\n\\n\\tMarginal variance\\n\\n\", V_tild)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitee8223ec65594bc885f48f30722f6205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
